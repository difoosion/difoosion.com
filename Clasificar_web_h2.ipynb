{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clasificar-web-h2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/difoosion/difoosion.com/blob/main/Clasificar_web_h2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapea y clasifica Web\n",
        "\n",
        "=> Scrapea una web y la clasifica por tÃ³picos. Muestra:\n",
        "\n",
        "- GrÃ¡fica de tÃ³picos de la web\n",
        "- GrÃ¡fica de dispersiÃ³n de los topicos\n",
        "- Tabla con palabras por tÃ³pico\n",
        "- Tabla de url por tÃ³pico con puntuaciÃ³n dentro del tÃ³pico.\n",
        "\n",
        "---\n",
        "1. Cambia en el cÃ³digo la url por la que quieras (ej. url = \"https://www.debelareabogados.es/\")\n",
        "2. Pulsa el play\n",
        "3. Espera a que el script acabe.\n",
        "4. Si salen demasiados tÃ³picos, en el Ãºltimo script selecciona el nÃºmero de tÃ³picos que quieres y dale al play en esa parte.\n",
        "\n",
        "** **Nota1**: Para webs enormes, los recursos de colab pueden quedarse cortos => En esos casos prueba a descargarte el archivo o el cÃ³digo y Ãºsalo en local con un ordenador potente.\n",
        "\n",
        "** **Nota2**: Se podrÃ­a adaptar ligeramente para que funcione en otros idiomas aparte de castellano cambiando la config de spacy. E incluso incluir la traducciÃ³n de categorias y titles al castellano.\n",
        "\n",
        "** **Nota3**: La idea inicial de este colab parte de [un artÃ­culo de holistic seo](https://www.holisticseo.digital/python-seo/topic-modeling/) \n",
        "\n",
        "Un saludo desde Malloca,\n",
        "\n",
        "[Jose Gris](https://twitter.com/JoseGrisSEO) ðŸ˜Ž\n",
        "\n",
        "â†“ El contenido aparecerÃ¡ debajo del script â†“"
      ],
      "metadata": {
        "id": "xNWbGRk_aHD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-KXECOEAZ7i",
        "outputId": "f8311324-9fb7-4bef-9fae-a1222f75ee6e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: advertools in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from advertools) (0.4.8)\n",
            "Requirement already satisfied: twython in /usr/local/lib/python3.7/dist-packages (from advertools) (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from advertools) (1.3.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from advertools) (6.0.1)\n",
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.7/dist-packages (from advertools) (2.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->advertools) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->advertools) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->advertools) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->advertools) (1.15.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (21.1.0)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (4.2.6)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (5.4.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.0.4)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (22.0.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.6.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (3.2.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (2.0.5)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (57.4.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (0.5.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.22.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (0.2.1)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (22.2.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (36.0.2)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.6.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy->advertools) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy->advertools) (2.21)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from itemloaders>=1.0.1->scrapy->advertools) (1.0.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy->advertools) (0.2.8)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy->advertools) (21.4.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (15.1.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (21.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (3.10.0.2)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (21.3.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (20.2.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy->advertools) (2.10)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy->advertools) (3.6.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy->advertools) (2.23.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy->advertools) (1.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy->advertools) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy->advertools) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy->advertools) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython->advertools) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython->advertools) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "url = \"https://www.elpradopsicologos.es/\" #ej url = 'https://www.debelareabogados.es/'\n",
        "!pip install advertools\n",
        "import pandas as pd\n",
        "from advertools import crawl\n",
        "\n",
        "!rm -rf output.jl\n",
        "crawl(url, 'web.jl', follow_links=True)\n",
        "\n",
        "web = pd.read_json(\"web.jl\", lines=True)\n",
        "def rotulos(rotulo):\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"---------------------------------\")\n",
        "  print(rotulo)\n",
        "  print(\"---------------------------------\")\n",
        "\n",
        "from google.colab import data_table\n",
        "def pasarATabla(dataframe0, Nlineas=10):\n",
        "  tabla = data_table.DataTable(dataframe0, include_index=False, num_rows_per_page=Nlineas)\n",
        "  display(tabla)\n",
        "\n",
        "def pasarListaATabla(lista,columnas):\n",
        "  lista = pd.DataFrame (lista, columns = columnas )\n",
        "  lista = data_table.DataTable(lista, include_index=True, num_rows_per_page=20)\n",
        "  display(lista)\n",
        "\n",
        "rotulos(\"CÃ³digo de respuestas distinto de 200\")\n",
        "noCodigo200 = web[web['status'] != 200][[\"url\", \"title\", \"status\"]]\n",
        "pasarATabla(noCodigo200)\n",
        "\n",
        "#Pillo lo que me interesa del scrapeado, y sÃ³lo cÃ³digos de respuesta 200\n",
        "codigos200 = web[web['status'] == 200][[\"url\",\"title\", \"h1\", \"h2\"]]\n",
        "\n",
        "#Me hago un corpus de datos con title y h2. Los h1 a menudo se acortan demasiado\n",
        "interesantes = [\"title\", \"h2\"]\n",
        "docs = []\n",
        "\n",
        "for param in interesantes:\n",
        "\n",
        "  espejo = codigos200[(codigos200[param].isna() ==False)][param].str.split(\"@@\").explode().drop_duplicates().to_list()\n",
        "  docs = docs + espejo\n",
        "  \n",
        "#Instalo spacy para quitar puntos, conjunciones, vertbos...\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "import spacy\n",
        "\n",
        "import re\n",
        "\n",
        "#quito acentos para igualar las faltas\n",
        "from unicodedata import normalize\n",
        "def quitarAcentos(frase):\n",
        "  frase = re.sub(\n",
        "        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
        "        normalize( \"NFD\", frase), 0, re.I\n",
        "    )\n",
        "  frase = normalize( 'NFC', frase)\n",
        "  return frase\n",
        "\n",
        "# Limpio el texto antes de procesarlo \n",
        "def limpiarTexto(texto):\n",
        "  texto = re.sub(r'\\s+', ' ', texto)\n",
        "  texto = re.sub(r'\\n+', ' ', texto)\n",
        "  texto = re.sub(r'\\t+', ' ', texto)\n",
        "  texto = re.sub(r'http\\S+', '', texto)\n",
        "  texto = re.sub('[^a-zA-ZÃ€-Ã¿\\u00f1\\u00d1,.Â¿?Â¡!]', ' ', texto)\n",
        "  doc = nlp(texto)\n",
        "  evitar = {\"DET\", \"CONJ\", \"CCONJ\", \"ADP\", \"ADV\", \"AUX\", \"PRON\", \"INTJ\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\"} \n",
        "  soloPalabras = [token.text #lemma_\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and token.pos_ not in evitar]\n",
        "  texto = \" \".join(soloPalabras)\n",
        "  texto = texto.replace(\"  \", \" \").strip()\n",
        "  texto = texto.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "  texto = quitarAcentos(texto).lower()\n",
        "  return texto\n",
        "\n",
        "docsLimpios = []\n",
        "for doc in docs:\n",
        "  docsLimpios.append(limpiarTexto(doc))\n",
        "\n",
        "# Me instalo BERTOPIC\n",
        "!pip install bertopic[gensim]\n",
        "from bertopic import BERTopic\n",
        "\n",
        "model = BERTopic(language=\"spanish\") #, nr_topics=\"auto\" reducirÃ­a topicos en auto\n",
        "\n",
        "topics, probabilities = model.fit_transform(docsLimpios)\n",
        "model.get_topic_info()\n",
        "\n",
        "#Como info...\n",
        "for param in interesantes:\n",
        "  #identifico los que tienen el parametro nulo\n",
        "  nulos = codigos200[(codigos200[param].isna())]\n",
        "  rotulos(f\"Sin {param}\")\n",
        "  pasarATabla(nulos)\n",
        "\n",
        "  #identifico los que tienen el parametro duplicado\n",
        "  duplicados = codigos200[(codigos200[param].isna()==False)]\n",
        "  duplicados = duplicados[(codigos200.duplicated(param))]\n",
        "  rotulos(f\"{param} duplicado\")\n",
        "  pasarATabla(duplicados.head())\n",
        "\n",
        "def recuperarTopico(doc):\n",
        "  doc0 = str(doc) \n",
        "  try:\n",
        "    #cat = similar_topic, similarity = model.find_topics(limpiarTexto(doc), top_n=1)\n",
        "    #cat1 = model.get_topic_info(cat[0][0])\n",
        "    #return cat1.Name.iloc[0] + \"___\"  + str(round(cat[1][0], 2))\n",
        "\n",
        "    similar_topic, similarity = model.find_topics(limpiarTexto(doc), top_n=1)\n",
        "    #topico = model.get_topic(similar_topic[0])\n",
        "    topicos = model.get_topic_info()\n",
        "    miTopico = topicos[topicos[\"Topic\"] == similar_topic[0]][\"Name\"].iloc[0]\n",
        "    return miTopico + \"___\"  + str(round(similarity[0], 2))\n",
        "\n",
        "  except:\n",
        "    return \"None\"\n",
        "\n",
        "def graficas2():\n",
        "  fig1 = model.visualize_barchart(n_words=6, top_n_topics=len(model.get_topics()))\n",
        "  fig1.show()\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  print(\"- Hijos de las SERPs, yo, yo soy Jose Gris!\")\n",
        "  print(\"- Hiiiiiiiiiiiii, hiiiiiiiiiiiiiiii (relincho de caballos)\")\n",
        "  print(\"- No puede ser, Jose Gris mide mÃ¡s de dos metros y dicen que es capaz de posicionar webs en Flash\")\n",
        "  print(\"- Â¡VÃ¡monos, huyamos!\")\n",
        "  print(\"- Huid y vivireis, un tiempo al menos... Pero llegarÃ¡ un dÃ­a, en vuestro lecho de muerte, en el que lo darÃ­as todo, TODO por una oportunidad,\")\n",
        "  print(\"      UNA SOLA OPORTUNIDAAAADD, DE VOLVER A LAS SERPSSSSS Y LUCHAR POR POSICIONAAAAAAAAARRRRRRRR!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "  print(\"--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "  fig2 = model.visualize_topics()\n",
        "  fig2.show()\n",
        "  fig3  = model.visualize_hierarchy()\n",
        "  fig3.show()\n",
        "  \n",
        "  #Creo tabla con definiciÃ³n de todos los tÃ³picos\n",
        "  todosLosTopicos = model.get_topics()\n",
        "  topicos = []\n",
        "  for topicoInt in todosLosTopicos:\n",
        "    topico = [x[0] for x in todosLosTopicos[topicoInt]]\n",
        "    topicos.append([topicoInt,\" - \".join(topico).strip().strip(\"-\")])\n",
        "  pasarListaATabla(topicos, [\"TÃ³pico\", \"DefiniciÃ³n\"])\n",
        "\n",
        "  #creo tabla clasificando urls por tÃ³pico\n",
        "  paramAComparar = \"h1\" #opcional clasificar por title\n",
        "  titulos0 = codigos200[[\"url\", paramAComparar]] \n",
        "  titulos0 = titulos0[titulos0[paramAComparar].isna() == False].drop_duplicates()\n",
        "  titulos0[\"Categoria\"] = titulos0.apply(lambda row: recuperarTopico(row[paramAComparar]), axis = 1)\n",
        "  titulos0[['Categoria', 'PuntuaciÃ³n']] = titulos0['Categoria'].str.split(\"___\", 1, expand=True)\n",
        "  titulos0 = titulos0.sort_values(['Categoria', 'PuntuaciÃ³n'], ascending=[True, False])\n",
        "  pasarATabla(titulos0, 20)\n",
        "\n",
        "graficas2()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "â†‘ Los resultados aparecerÃ¡n arriba => Puedes pulsar el botÃ³n de copiar en cada tabla para descargarlas\n",
        "\n",
        "â†“ Si quieres reducir el nÃºmero de tÃ³picos => cambia numTopicos al valor que quieras y pulsa el play en el cÃ³digo de abajo"
      ],
      "metadata": {
        "id": "8qqXm4FKa3G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OpciÃ³n reducir el nÃºmero de topicos\n",
        "numTopicos = 15\n",
        "new_topics, new_probs = model.reduce_topics(docsLimpios, topics, nr_topics=numTopicos)\n",
        "graficas2()"
      ],
      "metadata": {
        "id": "Xh9eBcFRfjUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}